{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "massive-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.svm import SVC \n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441852c",
   "metadata": {},
   "source": [
    "Step1: Prepare data for training and testing\n",
    "--------------------------------\n",
    "For SVM-Ar, the shared ```data``` folder only contains processed ```X_train```, ```y_trian```, ```X_test```, and ```y_test``` due to the upload size limit ISWC submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "associate-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60792\n",
      "65568\n",
      "76277\n",
      "60792 29509 31283\n",
      "65568 32068 33500\n",
      "(29509, 78) (31283, 78)\n",
      "(29509, 79) (31283, 79)\n",
      "(60792, 79)\n",
      "dumping X_train, y_train\n",
      "(32068, 78) (33500, 78)\n",
      "(32068, 79) (33500, 79)\n",
      "(65568, 79)\n",
      "CPU times: user 7min 4s, sys: 7.77 s, total: 7min 12s\n",
      "Wall time: 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "######### Extracted pattern list\n",
    "p78_list = ['8 -1',\n",
    " '17 -1',\n",
    " '15 -1',\n",
    " '29 -1',\n",
    " '6 -1',\n",
    " '4 -1',\n",
    " '22 -1',\n",
    " '25 -1',\n",
    " '12 -1',\n",
    " '5 -1',\n",
    " '3 -1',\n",
    " '16 -1',\n",
    " '14 -1',\n",
    " '6 -1 8 -1',\n",
    " '8 -1 8 -1',\n",
    " '8 -1 6 -1',\n",
    " '17 -1 8 -1',\n",
    " '8 -1 17 -1',\n",
    " '6 -1 6 -1',\n",
    " '18 -1 8 -1',\n",
    " '17 -1 6 -1',\n",
    " '6 -1 17 -1',\n",
    " '8 -1 18 -1',\n",
    " '15 -1 8 -1',\n",
    " '18 -1 6 -1',\n",
    " '17 -1 17 -1',\n",
    " '6 -1 8 -1 8 -1',\n",
    " '6 -1 6 -1 8 -1',\n",
    " '8 -1 8 -1 8 -1',\n",
    " '8 -1 8 -1 6 -1',\n",
    " '8 -1 6 -1 8 -1',\n",
    " '6 -1 8 -1 6 -1',\n",
    " '8 -1 6 -1 6 -1',\n",
    " '6 -1 6 -1 6 -1',\n",
    " '17 -1 8 -1 8 -1',\n",
    " '18 -1 8 -1 8 -1',\n",
    " '17 -1 6 -1 8 -1',\n",
    " '18 -1 6 -1 8 -1',\n",
    " '6 -1 17 -1 8 -1',\n",
    " '1 -1 24 -1',\n",
    " '11 -1 11 -1 13 -1',\n",
    " '2 -1 13 -1 26 -1',\n",
    " '17 -1 9 -1 13 -1',\n",
    " '24 -1 23 -1 22 -1',\n",
    " '25 -1 24 -1 17 -1',\n",
    " '2 -1 11 -1 13 -1',\n",
    " '24 -1 11 -1 15 -1',\n",
    " '24 -1 15 -1 11 -1',\n",
    " '20 -1 17 -1 24 -1',\n",
    " '29 -1 3 -1 24 -1',\n",
    " '21 -1 24 -1 22 -1',\n",
    " '21 -1 24 -1 5 -1',\n",
    " '24 -1 21 -1 26 -1',\n",
    " '24 -1 23 -1 26 -1',\n",
    " '7 -1 2 -1 25 -1',\n",
    " '3 -1 14 -1 24 -1',\n",
    " '24 -1 2 -1 2 -1',\n",
    " '15 -1 3 -1 24 -1',\n",
    " '13 -1 2 -1 14 -1',\n",
    " '3 -1 4 -1 24 -1',\n",
    " '4 -1 3 -1 24 -1',\n",
    " '13 -1 2 -1 13 -1',\n",
    " '24 -1 15 -1 2 -1',\n",
    " '21 -1 24 -1 26 -1',\n",
    " '24 -1 14 -1 28 -1',\n",
    " '4 -1 24 -1 13 -1',\n",
    " '9 -1 17 -1 13 -1',\n",
    " '24 -1 21 -1 2 -1',\n",
    " '2 -1 13 -1 14 -1',\n",
    " '25 -1 24 -1 14 -1',\n",
    " '23 -1 7 -1 2 -1',\n",
    " '3 -1 29 -1 24 -1',\n",
    " '21 -1 24 -1 2 -1',\n",
    " '1 -1 27 -1 17 -1',\n",
    " '24 -1 23 -1 2 -1',\n",
    " '27 -1 11 -1 14 -1',\n",
    " '12 -1 3 -1 24 -1',\n",
    " '2 -1 27 -1 20 -1']\n",
    "\n",
    "\n",
    "# load train users\n",
    "with open('../data/a_users_s2.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        train_users = pickle.load(filehandle)\n",
    "        print(len(train_users))\n",
    "\n",
    "# test\n",
    "with open('../data/a_users_s3.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        test_users = pickle.load(filehandle)\n",
    "        print(len(test_users))\n",
    "            \n",
    "# test\n",
    "with open('../data/a_users_s4.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        active_test_users = pickle.load(filehandle)\n",
    "        print(len(active_test_users))\n",
    "            \n",
    "# train-test to index\n",
    "with open('../data/train-test-users.pkl', 'rb') as f:\n",
    "        distinct_users = pickle.load(f)\n",
    "        \n",
    "# train_users active\n",
    "train_u_active = [x for x in train_users if x in test_users]\n",
    "# train users inactive\n",
    "train_u_inactive = [x for x in train_users if x not in test_users]\n",
    "print(len(train_users),len(train_u_active),len(train_u_inactive))\n",
    "\n",
    "# test_users active\n",
    "test_u_active = [x for x in test_users if x in active_test_users]\n",
    "# test users inactive\n",
    "test_u_inactive = [x for x in test_users if x not in active_test_users]\n",
    "print(len(test_users),len(test_u_active),len(test_u_inactive))\n",
    "\n",
    "\n",
    "######### Training data preparation\n",
    "if os.path.exists('X_train.pkl'):\n",
    "    with open('X_train.pkl','rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open('y_train.pkl','rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "else:\n",
    "    act_p_dict = dict()\n",
    "    inact_p_dict = dict()\n",
    "    act_p_ind_dict = dict()\n",
    "    inact_p_ind_dict = dict()\n",
    "    all_patterns = list()\n",
    "\n",
    "    with open('/media/parklize/Elements/dataset/train-spmf/train-active-spmf-output.txt', 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            pattern = l[:l.find(' #SUP')].strip()\n",
    "            count = l[l.find('#SUP: ')+5:l.find(' #SID')].strip()\n",
    "            indices = [int(x) for x in l[l.find('#SID: ')+5:].strip().split(' ')]\n",
    "            act_p_ind_dict[pattern] = indices\n",
    "            act_p_dict[pattern] = int(count)\n",
    "            all_patterns.append(pattern)\n",
    "\n",
    "    with open('/media/parklize/Elements/dataset/train-spmf/train-inactive-spmf-output.txt', 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            pattern = l[:l.find(' #SUP')].strip()\n",
    "            count = l[l.find('#SUP: ')+5:l.find(' #SID')].strip()\n",
    "            indices = [int(x) for x in l[l.find('#SID: ')+5:].strip().split(' ')]\n",
    "            inact_p_ind_dict[pattern] = indices\n",
    "            inact_p_dict[pattern] = int(count)\n",
    "            all_patterns.append(pattern)\n",
    "\n",
    "    all_patterns = list(set(all_patterns))\n",
    "\n",
    "    #::::: inactive/active # * 78 + 1 label for training \n",
    "    # active\n",
    "    act_np = np.zeros(shape=(29509,78))\n",
    "    for ind, p in enumerate(p78_list):\n",
    "        act_np[act_p_ind_dict[p],ind] = 1.\n",
    "\n",
    "    # inactive (last 8 is single record without pattern)\n",
    "    inact_np = np.zeros(shape=(31283,78))\n",
    "    for ind, p in enumerate(p78_list):\n",
    "        if p in inact_p_ind_dict:\n",
    "            inact_np[inact_p_ind_dict[p],ind] = 1.\n",
    "\n",
    "    print(act_np.shape, inact_np.shape)\n",
    "\n",
    "    # attache ylabels (inactive 1, active 0)\n",
    "    act_np = np.concatenate([act_np, np.zeros(shape=(29509,1))], axis=1)\n",
    "    inact_np = np.concatenate([inact_np, np.ones(shape=(31283,1))], axis=1)\n",
    "    print(act_np.shape, inact_np.shape)\n",
    "\n",
    "    # combine\n",
    "    train_np = np.concatenate([act_np, inact_np], axis=0)\n",
    "    print(train_np.shape)\n",
    "\n",
    "    X_train = train_np[:,:78]\n",
    "    y_train = train_np[:,-1]\n",
    "\n",
    "    print('dumping X_train, y_train')\n",
    "    with open('X_train.pkl','wb') as f:\n",
    "        pickle.dump(X_train, f)\n",
    "        \n",
    "    with open('y_train.pkl','wb') as f:\n",
    "        pickle.dump(y_train, f)\n",
    "        \n",
    "        \n",
    "############## Test data preparation\n",
    "if os.path.exists('X_test.pkl'):\n",
    "    with open('X_test.pkl','rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open('y_test.pkl','rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "else:\n",
    "    act_p_dict = dict()\n",
    "    inact_p_dict = dict()\n",
    "    act_p_ind_dict = dict()\n",
    "    inact_p_ind_dict = dict()\n",
    "\n",
    "    with open('/media/parklize/Elements/dataset/train-spmf/test-active-spmf-output.txt', 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            pattern = l[:l.find(' #SUP')].strip()\n",
    "            count = l[l.find('#SUP: ')+5:l.find(' #SID')].strip()\n",
    "            indices = [int(x) for x in l[l.find('#SID: ')+5:].strip().split(' ')]\n",
    "            act_p_ind_dict[pattern] = indices\n",
    "            act_p_dict[pattern] = int(count)\n",
    "\n",
    "    with open('/media/parklize/Elements/dataset/train-spmf/test-inactive-spmf-output.txt', 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            pattern = l[:l.find(' #SUP')].strip()\n",
    "            count = l[l.find('#SUP: ')+5:l.find(' #SID')].strip()\n",
    "            indices = [int(x) for x in l[l.find('#SID: ')+5:].strip().split(' ')]\n",
    "            inact_p_ind_dict[pattern] = indices\n",
    "            inact_p_dict[pattern] = int(count)\n",
    "            \n",
    "    #::::: inactive/active # * 78  for testing \n",
    "    # active\n",
    "    act_np = np.zeros(shape=(32068,78))\n",
    "    for ind, p in enumerate(p78_list):\n",
    "        act_np[act_p_ind_dict[p],ind] = 1.\n",
    "\n",
    "    # inactive (last 3 is single record without pattern)\n",
    "    inact_np = np.zeros(shape=(33500,78))\n",
    "    for ind, p in enumerate(p78_list):\n",
    "        if p in inact_p_ind_dict:\n",
    "            inact_np[inact_p_ind_dict[p],ind] = 1.\n",
    "\n",
    "    print(act_np.shape, inact_np.shape)\n",
    "\n",
    "    # attache ylabels (inactive 1, active 0)\n",
    "    act_np = np.concatenate([act_np, np.zeros(shape=(32068,1))], axis=1)\n",
    "    inact_np = np.concatenate([inact_np, np.ones(shape=(33500,1))], axis=1)\n",
    "    print(act_np.shape, inact_np.shape)\n",
    "\n",
    "    # combine\n",
    "    test_np = np.concatenate([act_np, inact_np], axis=0)\n",
    "    print(test_np.shape)\n",
    "\n",
    "    X_test = test_np[:,:78]\n",
    "    y_test = test_np[:,-1]\n",
    "    \n",
    "    print('dumping X_test, y_test')\n",
    "    with open('X_test.pkl','wb') as f:\n",
    "        pickle.dump(X_test, f)\n",
    "        \n",
    "    with open('y_test.pkl','wb') as f:\n",
    "        pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-chart",
   "metadata": {},
   "source": [
    "Step2: Training SVM-Ar\n",
    "--------------\n",
    "(Go to Step3: Testing SVM-Ar to run the one already trained for the paper)\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "forbidden-figure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 7s, sys: 1.1 s, total: 35min 8s\n",
      "Wall time: 35min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, gamma=0.1, probability=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Gridsearch CV\n",
    "# # defining parameter range \n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['rbf']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(probability=True), param_grid, refit = True, verbose=3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) \n",
    "\n",
    "# clf = grid.best_estimator_\n",
    "# print(clf) # SVC(C=0.1, gamma=0.1)\n",
    "\n",
    "# use gridsearch result directly for retraining\n",
    "clf = SVC(probability=True, C=0.1, gamma=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "dump(clf, 'tmp/svm.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-remark",
   "metadata": {},
   "source": [
    "Step2: Testing SVM-Ar\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "photographic-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained SVM model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7994    0.7785    0.7888     32068\n",
      "         1.0     0.7931    0.8130    0.8029     33500\n",
      "\n",
      "    accuracy                         0.7961     65568\n",
      "   macro avg     0.7963    0.7957    0.7959     65568\n",
      "weighted avg     0.7962    0.7961    0.7960     65568\n",
      "\n",
      "AUROC 0.8396467166785506\n",
      "Log Loss 0.488075968530016\n",
      "CPU times: user 12min 45s, sys: 817 ms, total: 12min 46s\n",
      "Wall time: 13min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = load('svm.joblib') \n",
    "print('loaded trained SVM model...')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "print('AUROC', metrics.auc(fpr, tpr))\n",
    "print('Log Loss', metrics.log_loss(y_test, y_prob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
