{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "moral-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from deepctr.models import xDeepFM, DeepFM\n",
    "from deepctr.feature_column import  SparseFeat, DenseFeat, get_feature_names\n",
    "import tensorflow\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.python.keras.metrics import AUC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reproducable\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tensorflow.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-yugoslavia",
   "metadata": {},
   "source": [
    "DeepFM-Stat\n",
    "=======================\n",
    "\n",
    "Step1: Prepare data for training and testing DeepFM-Stat\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "realistic-coaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60792\n",
      "65568\n",
      "76277\n",
      "60792 29509 31283\n",
      "65568 32068 33500\n",
      "[0, 1, 2, 6, 7, 8, 12, 13, 14, 18, 19, 20, 24, 25, 26, 30, 31, 32, 36, 37, 38, 42, 43, 44, 48, 49, 50, 54, 55, 56, 60, 61]\n",
      "(60792, 62) (60792,)\n",
      "(65568, 82)\n",
      "(65568, 62) (65568,)\n",
      "CPU times: user 15min 53s, sys: 607 ms, total: 15min 54s\n",
      "Wall time: 16min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load train users\n",
    "with open('../data/a_users_s2.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        train_users = pickle.load(filehandle)\n",
    "        print(len(train_users))\n",
    "\n",
    "# test\n",
    "with open('../data/a_users_s3.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        test_users = pickle.load(filehandle)\n",
    "        print(len(test_users))\n",
    "            \n",
    "# test\n",
    "with open('../data/a_users_s4.data', 'rb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        active_test_users = pickle.load(filehandle)\n",
    "        print(len(active_test_users))\n",
    "        \n",
    "            \n",
    "# train-test to index\n",
    "with open('../data/train-test-users.pkl', 'rb') as f:\n",
    "        distinct_users = pickle.load(f)\n",
    "        \n",
    "# train_users active\n",
    "train_u_active = [x for x in train_users if x in test_users]\n",
    "# train users inactive\n",
    "train_u_inactive = [x for x in train_users if x not in test_users]\n",
    "print(len(train_users),len(train_u_active),len(train_u_inactive))\n",
    "\n",
    "# test_users active\n",
    "test_u_active = [x for x in test_users if x in active_test_users]\n",
    "# test users inactive\n",
    "test_u_inactive = [x for x in test_users if x not in active_test_users]\n",
    "print(len(test_users),len(test_u_active),len(test_u_inactive))\n",
    "\n",
    "\n",
    "######### Training data preparation\n",
    "# load\n",
    "with open('../data/ours_traindf_list.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "# display(train_df)\n",
    "\n",
    "    \n",
    "### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy']\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "train_df = train_df[columns_consider]\n",
    "# display(train_df)\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "print(non_entropy_col_indices)\n",
    "###\n",
    "    \n",
    "# y_train\n",
    "y_labels = [x in test_users for x in train_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': train_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "train_df = pd.concat([train_df, y_df], axis=1)\n",
    "train_df = train_df.fillna(0)\n",
    "train_df.shape\n",
    "\n",
    "# X_train\n",
    "X_train_ = train_df.values[:,:-1]\n",
    "y_train = train_df.values[:,-1 ]\n",
    "y_train = 1 - y_train # 0 for active\n",
    "np.nan_to_num(X_train_, copy=False, nan=0)\n",
    "\n",
    "X_train_ = X_train_.astype(dtype=np.float32)\n",
    "y_train = y_train.astype(dtype=np.int8)\n",
    "X_train = np.log10(X_train_+1.)\n",
    "\n",
    "# shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=28)\n",
    "# # transform\n",
    "# mms = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_train = mms.fit_transform(X_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "############# Test data preparation\n",
    "with open('../data/ours_testdf_list.pkl', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    test_df = pickle.load(filehandle)\n",
    "    \n",
    "print(test_df.shape)\n",
    "\n",
    "#### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy']\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "test_df = test_df[columns_consider]\n",
    "# display(test_df.head())\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "####\n",
    "\n",
    "\n",
    "# y_test\n",
    "y_labels = [x in active_test_users for x in test_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': test_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "test_df = pd.concat([test_df, y_df], axis=1)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "# display(test_df.head())\n",
    "\n",
    "# X_test\n",
    "X_test_ = test_df.values[:,:-1]\n",
    "y_test = test_df.values[:,-1]\n",
    "y_test = 1-y_test # 1 for inactive\n",
    "np.nan_to_num(X_test_, copy=False, nan=0)\n",
    "\n",
    "X_test = X_test_.astype(dtype=np.float32)\n",
    "y_test = y_test.astype(dtype=np.int8)\n",
    "X_test = np.log10(X_test+1.)\n",
    "\n",
    "# transform\n",
    "# X_test = mms.transform(X_test)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-associate",
   "metadata": {},
   "source": [
    "Step2: Training DeepFM-Stat\n",
    "--------------\n",
    "(Go to Step3: Testing DeepFM to run the one already trained for the paper)\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "broken-waters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of dense features 62\n",
      "# of params 24767\n",
      "Epoch 1/1000\n",
      "190/190 - 13s - loss: 0.4557 - auc: 0.8611 - val_loss: 0.4371 - val_auc: 0.8757\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.87573, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 2/1000\n",
      "190/190 - 5s - loss: 0.4262 - auc: 0.8820 - val_loss: 0.4300 - val_auc: 0.8820\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.87573 to 0.88202, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 3/1000\n",
      "190/190 - 4s - loss: 0.4191 - auc: 0.8864 - val_loss: 0.4221 - val_auc: 0.8850\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.88202 to 0.88502, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 4/1000\n",
      "190/190 - 4s - loss: 0.4148 - auc: 0.8888 - val_loss: 0.4257 - val_auc: 0.8853\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.88502 to 0.88533, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 5/1000\n",
      "190/190 - 4s - loss: 0.4146 - auc: 0.8889 - val_loss: 0.4246 - val_auc: 0.8865\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.88533 to 0.88654, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 6/1000\n",
      "190/190 - 4s - loss: 0.4128 - auc: 0.8899 - val_loss: 0.4228 - val_auc: 0.8867\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.88654 to 0.88668, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 7/1000\n",
      "190/190 - 3s - loss: 0.4132 - auc: 0.8896 - val_loss: 0.4205 - val_auc: 0.8869\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.88668 to 0.88686, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 8/1000\n",
      "190/190 - 4s - loss: 0.4127 - auc: 0.8899 - val_loss: 0.4215 - val_auc: 0.8873\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.88686 to 0.88734, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 9/1000\n",
      "190/190 - 4s - loss: 0.4125 - auc: 0.8900 - val_loss: 0.4257 - val_auc: 0.8869\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.88734\n",
      "Epoch 10/1000\n",
      "190/190 - 4s - loss: 0.4116 - auc: 0.8905 - val_loss: 0.4180 - val_auc: 0.8876\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.88734 to 0.88756, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 11/1000\n",
      "190/190 - 4s - loss: 0.4112 - auc: 0.8907 - val_loss: 0.4177 - val_auc: 0.8875\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.88756\n",
      "Epoch 12/1000\n",
      "190/190 - 4s - loss: 0.4106 - auc: 0.8911 - val_loss: 0.4167 - val_auc: 0.8878\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.88756 to 0.88777, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 13/1000\n",
      "190/190 - 4s - loss: 0.4096 - auc: 0.8916 - val_loss: 0.4169 - val_auc: 0.8875\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.88777\n",
      "Epoch 14/1000\n",
      "190/190 - 4s - loss: 0.4101 - auc: 0.8913 - val_loss: 0.4170 - val_auc: 0.8874\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.88777\n",
      "Epoch 15/1000\n",
      "190/190 - 4s - loss: 0.4098 - auc: 0.8914 - val_loss: 0.4276 - val_auc: 0.8868\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.88777\n",
      "Epoch 16/1000\n",
      "190/190 - 4s - loss: 0.4104 - auc: 0.8910 - val_loss: 0.4168 - val_auc: 0.8877\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.88777\n",
      "Epoch 17/1000\n",
      "190/190 - 4s - loss: 0.4095 - auc: 0.8916 - val_loss: 0.4196 - val_auc: 0.8869\n",
      "\n",
      "Epoch 00017: val_auc did not improve from 0.88777\n",
      "Epoch 18/1000\n",
      "190/190 - 4s - loss: 0.4089 - auc: 0.8919 - val_loss: 0.4204 - val_auc: 0.8868\n",
      "\n",
      "Epoch 00018: val_auc did not improve from 0.88777\n",
      "Epoch 19/1000\n",
      "190/190 - 4s - loss: 0.4082 - auc: 0.8923 - val_loss: 0.4166 - val_auc: 0.8879\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.88777 to 0.88787, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 20/1000\n",
      "190/190 - 5s - loss: 0.4081 - auc: 0.8924 - val_loss: 0.4179 - val_auc: 0.8872\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.88787\n",
      "Epoch 21/1000\n",
      "190/190 - 4s - loss: 0.4081 - auc: 0.8923 - val_loss: 0.4185 - val_auc: 0.8873\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.88787\n",
      "Epoch 22/1000\n",
      "190/190 - 4s - loss: 0.4078 - auc: 0.8926 - val_loss: 0.4167 - val_auc: 0.8877\n",
      "\n",
      "Epoch 00022: val_auc did not improve from 0.88787\n",
      "Epoch 23/1000\n",
      "190/190 - 4s - loss: 0.4075 - auc: 0.8928 - val_loss: 0.4166 - val_auc: 0.8878\n",
      "\n",
      "Epoch 00023: val_auc did not improve from 0.88787\n",
      "Epoch 24/1000\n",
      "190/190 - 7s - loss: 0.4069 - auc: 0.8929 - val_loss: 0.4181 - val_auc: 0.8874\n",
      "\n",
      "Epoch 00024: val_auc did not improve from 0.88787\n",
      "Epoch 25/1000\n",
      "190/190 - 5s - loss: 0.4075 - auc: 0.8926 - val_loss: 0.4173 - val_auc: 0.8879\n",
      "\n",
      "Epoch 00025: val_auc improved from 0.88787 to 0.88794, saving model to DeepFM_w_seed28_b1_all_features.h5\n",
      "Epoch 26/1000\n",
      "190/190 - 4s - loss: 0.4060 - auc: 0.8934 - val_loss: 0.4233 - val_auc: 0.8871\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.88794\n",
      "Epoch 27/1000\n",
      "190/190 - 4s - loss: 0.4073 - auc: 0.8927 - val_loss: 0.4193 - val_auc: 0.8864\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.88794\n",
      "Epoch 28/1000\n",
      "190/190 - 4s - loss: 0.4067 - auc: 0.8932 - val_loss: 0.4239 - val_auc: 0.8851\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.88794\n",
      "Epoch 29/1000\n",
      "190/190 - 5s - loss: 0.4064 - auc: 0.8932 - val_loss: 0.4230 - val_auc: 0.8862\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.88794\n",
      "Epoch 30/1000\n",
      "190/190 - 4s - loss: 0.4066 - auc: 0.8932 - val_loss: 0.4191 - val_auc: 0.8866\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.88794\n",
      "Epoch 31/1000\n",
      "190/190 - 4s - loss: 0.4053 - auc: 0.8939 - val_loss: 0.4183 - val_auc: 0.8873\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.88794\n",
      "Epoch 32/1000\n",
      "190/190 - 4s - loss: 0.4060 - auc: 0.8935 - val_loss: 0.4220 - val_auc: 0.8871\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.88794\n",
      "Epoch 33/1000\n",
      "190/190 - 4s - loss: 0.4055 - auc: 0.8936 - val_loss: 0.4198 - val_auc: 0.8871\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.88794\n",
      "Epoch 34/1000\n",
      "190/190 - 4s - loss: 0.4052 - auc: 0.8938 - val_loss: 0.4186 - val_auc: 0.8871\n",
      "\n",
      "Epoch 00034: val_auc did not improve from 0.88794\n",
      "Epoch 35/1000\n",
      "190/190 - 4s - loss: 0.4050 - auc: 0.8939 - val_loss: 0.4192 - val_auc: 0.8878\n",
      "\n",
      "Epoch 00035: val_auc did not improve from 0.88794\n",
      "CPU times: user 4min 16s, sys: 3min 40s, total: 7min 56s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_all_features.h5'\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns = ['f'+str(i) for i in range(X_train.shape[1])])\n",
    "\n",
    "###\n",
    "dense_features = X_train_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "print('# of dense features', len(dense_features))\n",
    "train_model_input = {name: X_train_df[name].values for name in dense_features}\n",
    "###\n",
    "\n",
    "##### define model\n",
    "tensorflow.keras.backend.clear_session()\n",
    "model = DeepFM(dense_feature_columns, dense_feature_columns, \n",
    "               task='binary', seed=28, \n",
    "#                dnn_dropout=0.2, \n",
    "#                dnn_hidden_units=(128,128,64)\n",
    "              )\n",
    "print('# of params',model.count_params())\n",
    "\n",
    "# compiling the model\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', patience=10)\n",
    "mc = ModelCheckpoint(model_weight_path, \n",
    "                     monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[AUC()], )\n",
    "\n",
    "# training the model\n",
    "history = model.fit(train_model_input, y_train, batch_size=256, epochs=1000, \n",
    "                    validation_split=0.2, verbose=2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-andrews",
   "metadata": {},
   "source": [
    "Step3: Testing DeepFM-Stat\n",
    "--------------------\n",
    "The trained model weights for the 5 runs for the main article are in the same folder, which you can use for setting ```model_weight_path``` varialble in the cell below. \n",
    "- DeepFM_w_seed28_b1_all_features_r1.h5\n",
    "- DeepFM_w_seed28_b1_all_features_r2.h5\n",
    "- DeepFM_w_seed28_b1_all_features_r3.h5\n",
    "- DeepFM_w_seed28_b1_all_features_r4.h5\n",
    "- DeepFM_w_seed28_b1_all_features_r5.h5\n",
    "  \n",
    "In case of using newly trained model weight, you can use set the same model_weight_path in the Step2: Training DeepFM\n",
    "- DeepFM_w_seed28_b1_all_features.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "emerging-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65568, 62)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8437    0.7565    0.7977     32068\n",
      "           1     0.7879    0.8659    0.8250     33500\n",
      "\n",
      "    accuracy                         0.8124     65568\n",
      "   macro avg     0.8158    0.8112    0.8114     65568\n",
      "weighted avg     0.8152    0.8124    0.8117     65568\n",
      "\n",
      "AUROC 0.892900611387369\n",
      "Log Loss 0.4104401888362103\n",
      "0.8437\t0.7565\t0.7977\t0.7879\t0.8659\t0.8250\t0.892900611387369\t0.8124\t0.4104401888362103\n",
      "CPU times: user 8.8 s, sys: 6.9 s, total: 15.7 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_all_features_r1.h5'\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns = ['f'+str(i) for i in range(X_test.shape[1])])\n",
    "print(X_test_df.shape)\n",
    "\n",
    "dense_features = X_test_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "test_model_input = {name: X_test_df[name].values for name in dense_features}\n",
    "clf = DeepFM(dense_feature_columns, dense_feature_columns, \n",
    "             task='binary', seed=28, \n",
    "#              dnn_hidden_units=(128, 128, 64)\n",
    "            )\n",
    "\n",
    "clf.load_weights(model_weight_path)\n",
    "y_prob = clf.predict(test_model_input, batch_size=256).ravel()\n",
    "y_pred = [int(x) for x in (y_prob>=.5)]\n",
    "# print(y_pred)\n",
    "# print(y_prob)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "print('AUROC', metrics.auc(fpr, tpr))\n",
    "print('Log Loss', metrics.log_loss(y_test, y_prob))\n",
    "\n",
    "# easy to paste version\n",
    "res = classification_report(y_test, y_pred, digits=4).split('\\n')\n",
    "label0 = [x for x in res[2].split(' ') if len(x)>1][:3]\n",
    "label1 = [x for x in res[3].split(' ') if len(x)>1][:3]\n",
    "acc = [x for x in res[5].split(' ') if len(x)>1][1]\n",
    "res = label0+label1+[str(metrics.auc(fpr, tpr)), str(acc), str(metrics.log_loss(y_test, y_prob))]\n",
    "print('\\t'.join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sigma",
   "metadata": {},
   "source": [
    "DeepFM-Stat+Pattern\n",
    "========================\n",
    "\n",
    "Step1: Preparing data for training and testing DeepFM-Stat+Pattern\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "intense-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 6, 7, 8, 12, 13, 14, 18, 19, 20, 24, 25, 26, 30, 31, 32, 36, 37, 38, 42, 43, 44, 48, 49, 50, 54, 55, 56, 60, 61]\n",
      "(60792, 140)\n",
      "(60792, 140) (60792,)\n",
      "(65568, 82)\n",
      "(65568, 140)\n",
      "CPU times: user 2min 27s, sys: 1.83 s, total: 2min 29s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "############# Training data preparation\n",
    "# load train_df / test_df from SPMF\n",
    "train_df_spmf = pd.read_csv('../baseline3/train_df_spmf.csv')\n",
    "train_df_spmf.set_index('user',inplace=True)\n",
    "\n",
    "# load train / test for DeepFM\n",
    "with open('../data/ours_traindf_list.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "# display(train_df)\n",
    "\n",
    "### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy'\n",
    "]\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "train_df = train_df[columns_consider]\n",
    "# display(train_df)\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "print(non_entropy_col_indices)\n",
    "    \n",
    "# y_train\n",
    "y_labels = [x in test_users for x in train_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': train_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "# Concat two types of features\n",
    "train_df = pd.concat([\n",
    "    train_df, \n",
    "    train_df_spmf, y_df], axis=1)\n",
    "train_df = train_df.fillna(0)\n",
    "train_df.shape\n",
    "\n",
    "\n",
    "# X_train\n",
    "X_train_ = train_df.values[:,:-1]\n",
    "y_train = train_df.values[:,-1 ]\n",
    "y_train = 1 - y_train # 0 for active\n",
    "np.nan_to_num(X_train_, copy=False, nan=0)\n",
    "\n",
    "X_train_ = X_train_.astype(dtype=np.float32)\n",
    "y_train = y_train.astype(dtype=np.int8)\n",
    "# X_train = np.log10(X_train_+1.)\n",
    "\n",
    "X_train = X_train_\n",
    "X_train[:,:62] = np.log10(X_train[:,:62]+1.) # only log on non-spmf features\n",
    "print(X_train.shape)\n",
    "\n",
    "# shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=28)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "########## Test data preparation\n",
    "# load train_df / test_df from SPMF\n",
    "test_df_spmf = pd.read_csv('../baseline3/test_df_spmf.csv')\n",
    "test_df_spmf.set_index('user',inplace=True)\n",
    "\n",
    "# load deepfm features test\n",
    "with open('../data/ours_testdf_list.pkl', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    test_df = pickle.load(filehandle)\n",
    "print(test_df.shape)\n",
    "\n",
    "#### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy'\n",
    "]\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "test_df = test_df[columns_consider]\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "\n",
    "\n",
    "# y_test\n",
    "y_labels = [x in active_test_users for x in test_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': test_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "# Concat two types of features\n",
    "test_df = pd.concat([\n",
    "    test_df, \n",
    "    test_df_spmf, y_df], axis=1)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "# X_test\n",
    "X_test_ = test_df.values[:,:-1]\n",
    "y_test = test_df.values[:,-1]\n",
    "y_test = 1-y_test # 1 for inactive\n",
    "np.nan_to_num(X_test_, copy=False, nan=0)\n",
    "\n",
    "###\n",
    "X_test_ = X_test_.astype(dtype=np.float32)\n",
    "y_test = y_test.astype(dtype=np.int8)\n",
    "# X_train = np.log10(X_train_+1.)\n",
    "X_test = X_test_\n",
    "X_test[:,:62] = np.log10(X_test[:,:62]+1.) # only log on non-spmf features\n",
    "print(X_test.shape)\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea979edb",
   "metadata": {},
   "source": [
    "Step2: Training DeepFM-Stat+Pattern\n",
    "--------------\n",
    "(Go to Step3: Testing DeepFM-Stat+Pattern to run the one already trained for the paper)\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sized-falls",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of dense features 140\n",
      "# of params 65483\n",
      "Epoch 1/1000\n",
      "190/190 - 45s - loss: 0.3643 - auc: 0.9149 - val_loss: 0.3205 - val_auc: 0.9369\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.93685, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 2/1000\n",
      "190/190 - 15s - loss: 0.2923 - auc: 0.9476 - val_loss: 0.2932 - val_auc: 0.9492\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.93685 to 0.94925, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 3/1000\n",
      "190/190 - 15s - loss: 0.2702 - auc: 0.9555 - val_loss: 0.2711 - val_auc: 0.9558\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.94925 to 0.95576, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 4/1000\n",
      "190/190 - 15s - loss: 0.2537 - auc: 0.9608 - val_loss: 0.2620 - val_auc: 0.9584\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.95576 to 0.95839, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 5/1000\n",
      "190/190 - 15s - loss: 0.2464 - auc: 0.9630 - val_loss: 0.2554 - val_auc: 0.9603\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.95839 to 0.96031, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 6/1000\n",
      "190/190 - 15s - loss: 0.2401 - auc: 0.9647 - val_loss: 0.2550 - val_auc: 0.9614\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.96031 to 0.96135, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 7/1000\n",
      "190/190 - 15s - loss: 0.2351 - auc: 0.9661 - val_loss: 0.2486 - val_auc: 0.9622\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.96135 to 0.96221, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 8/1000\n",
      "190/190 - 14s - loss: 0.2309 - auc: 0.9672 - val_loss: 0.2482 - val_auc: 0.9632\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.96221 to 0.96321, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 9/1000\n",
      "190/190 - 15s - loss: 0.2277 - auc: 0.9681 - val_loss: 0.2473 - val_auc: 0.9624\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.96321\n",
      "Epoch 10/1000\n",
      "190/190 - 14s - loss: 0.2263 - auc: 0.9684 - val_loss: 0.2421 - val_auc: 0.9644\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.96321 to 0.96436, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 11/1000\n",
      "190/190 - 15s - loss: 0.2227 - auc: 0.9693 - val_loss: 0.2562 - val_auc: 0.9628\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.96436\n",
      "Epoch 12/1000\n",
      "190/190 - 15s - loss: 0.2204 - auc: 0.9699 - val_loss: 0.2474 - val_auc: 0.9643\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.96436\n",
      "Epoch 13/1000\n",
      "190/190 - 15s - loss: 0.2186 - auc: 0.9704 - val_loss: 0.2503 - val_auc: 0.9640\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.96436\n",
      "Epoch 14/1000\n",
      "190/190 - 15s - loss: 0.2163 - auc: 0.9710 - val_loss: 0.2400 - val_auc: 0.9648\n",
      "\n",
      "Epoch 00014: val_auc improved from 0.96436 to 0.96476, saving model to DeepFM_w_seed28_b1_all_features_spmfsparse.h5\n",
      "Epoch 15/1000\n",
      "190/190 - 15s - loss: 0.2145 - auc: 0.9714 - val_loss: 0.2420 - val_auc: 0.9643\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.96476\n",
      "Epoch 16/1000\n",
      "190/190 - 15s - loss: 0.2119 - auc: 0.9721 - val_loss: 0.2424 - val_auc: 0.9640\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.96476\n",
      "Epoch 17/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/py3.7tf2.4.1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_all_features_spmfsparse.h5'\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns = ['f'+str(i) for i in range(X_train.shape[1])])\n",
    "\n",
    "###\n",
    "dense_features = X_train_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "print('# of dense features', len(dense_features))\n",
    "train_model_input = {name: X_train_df[name].values for name in dense_features}\n",
    "###\n",
    "\n",
    "### sparse features for DeepFM for patterns\n",
    "sparse_features = [x for x in X_train_df.columns[62:]]\n",
    "dense_features = [x for x in X_train_df.columns[:62]]\n",
    "X_train_df[sparse_features] = X_train_df[sparse_features].astype(int)\n",
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=X_train_df[feat].max() + 1,embedding_dim=4)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "train_model_input = {name: X_train_df[name].values for name in feature_names}\n",
    "###\n",
    "\n",
    "\n",
    "##### define model\n",
    "tensorflow.keras.backend.clear_session()\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', seed=28)\n",
    "print('# of params',model.count_params())\n",
    "\n",
    "# compiling the model\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', patience=10)\n",
    "mc = ModelCheckpoint(model_weight_path, \n",
    "                     monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[AUC()], )\n",
    "# training the model\n",
    "history = model.fit(train_model_input, y_train, batch_size=256, epochs=1000, \n",
    "                    validation_split=0.2, verbose=2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7bad7",
   "metadata": {},
   "source": [
    "Step3: Testing DeepFM-Stat+Pattern\n",
    "------------------------\n",
    "The trained model weights for the 5 runs for the main article are in the same folder, which you can use for setting ```model_weight_path``` varialble in the cell below. \n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse_r1.h5\n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse_r2.h5\n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse_r3.h5\n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse_r4.h5\n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse_r5.h5\n",
    "  \n",
    "In case of using newly trained model weight, you can use set the same model_weight_path in the Step2: Training DeepFM\n",
    "- DeepFM_w_seed28_b1_all_features_spmfsparse.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "utility-firmware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65568, 140)\n",
      "140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8581    0.9178    0.8869     32068\n",
      "           1     0.9157    0.8547    0.8841     33500\n",
      "\n",
      "    accuracy                         0.8856     65568\n",
      "   macro avg     0.8869    0.8862    0.8855     65568\n",
      "weighted avg     0.8875    0.8856    0.8855     65568\n",
      "\n",
      "AUROC 0.9566304545936899\n",
      "Log Loss 0.2728058444851035\n",
      "CPU times: user 22.4 s, sys: 10.6 s, total: 33 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_all_features_spmfsparse_r1.h5'\n",
    "\n",
    "### DeepFM load and test\n",
    "X_test_df = pd.DataFrame(X_test, columns = ['f'+str(i) for i in range(X_test.shape[1])])\n",
    "print(X_test_df.shape)\n",
    "\n",
    "dense_features = X_test_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "test_model_input = {name: X_test_df[name].values for name in dense_features}\n",
    "###\n",
    "\n",
    "\n",
    "### sparse features for DeepFM for patterns\n",
    "sparse_features = [x for x in X_test_df.columns[62:]]\n",
    "dense_features = [x for x in X_test_df.columns[:62]]\n",
    "X_test_df[sparse_features] = X_test_df[sparse_features].astype(int)\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=X_test_df[feat].max() + 1,embedding_dim=4)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "print(len(feature_names))\n",
    "test_model_input = {name: X_test_df[name].values for name in feature_names}\n",
    "###\n",
    "\n",
    "clf = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', seed=28)\n",
    "clf.load_weights(model_weight_path)\n",
    "y_prob = clf.predict(test_model_input, batch_size=256).ravel()\n",
    "y_pred = [int(x) for x in (y_prob>=.5)]\n",
    "# print(y_pred)\n",
    "# print(y_prob)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob, pos_label=1)\n",
    "print('AUROC', metrics.auc(fpr, tpr))\n",
    "print('Log Loss', metrics.log_loss(y_test, y_prob, eps=1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe730895",
   "metadata": {},
   "source": [
    "DeepFM-Pattern\n",
    "========================\n",
    "\n",
    "Step1: Preparing data for training and testing DeepFM-Pattern\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############# Training data preparation\n",
    "# load train_df / test_df from SPMF\n",
    "train_df_spmf = pd.read_csv('../baseline3/train_df_spmf.csv')\n",
    "train_df_spmf.set_index('user',inplace=True)\n",
    "\n",
    "# load train / test for DeepFM\n",
    "with open('../data/ours_traindf_list.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "# display(train_df)\n",
    "\n",
    "### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy'\n",
    "]\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "train_df = train_df[columns_consider]\n",
    "# display(train_df)\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "print(non_entropy_col_indices)\n",
    "    \n",
    "# y_train\n",
    "y_labels = [x in test_users for x in train_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': train_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "# Concat two types of features\n",
    "train_df = pd.concat([\n",
    "    train_df, \n",
    "    train_df_spmf, y_df], axis=1)\n",
    "train_df = train_df.fillna(0)\n",
    "train_df.shape\n",
    "\n",
    "\n",
    "# X_train\n",
    "X_train_ = train_df.values[:,:-1]\n",
    "y_train = train_df.values[:,-1 ]\n",
    "y_train = 1 - y_train # 0 for active\n",
    "np.nan_to_num(X_train_, copy=False, nan=0)\n",
    "\n",
    "X_train_ = X_train_.astype(dtype=np.float32)\n",
    "y_train = y_train.astype(dtype=np.int8)\n",
    "# X_train = np.log10(X_train_+1.)\n",
    "\n",
    "X_train = X_train_\n",
    "X_train[:,:62] = np.log10(X_train[:,:62]+1.) # only log on non-spmf features\n",
    "print(X_train.shape)\n",
    "\n",
    "# shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=28)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "########## Test data preparation\n",
    "# load train_df / test_df from SPMF\n",
    "test_df_spmf = pd.read_csv('../baseline3/test_df_spmf.csv')\n",
    "test_df_spmf.set_index('user',inplace=True)\n",
    "\n",
    "# load deepfm features test\n",
    "with open('../data/ours_testdf_list.pkl', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    test_df = pickle.load(filehandle)\n",
    "print(test_df.shape)\n",
    "\n",
    "#### columns to consdier\n",
    "columns_model = [\n",
    "    'count',\n",
    "    'unique',\n",
    "    'daydiff',\n",
    "    'edittype_entropy',\n",
    "    'target_entropy',\n",
    "    'dow_entropy'\n",
    "]\n",
    "columns_consider = list()\n",
    "for i in range(10):\n",
    "    for c in columns_model:\n",
    "        columns_consider.append((i, c))\n",
    "columns_consider += [\n",
    "    (9,'pred_first_edit'),\n",
    "    (9,'pred_last_edit')\n",
    "]\n",
    "test_df = test_df[columns_consider]\n",
    "non_entropy_col_indices = list()\n",
    "for i,c in enumerate(columns_consider):\n",
    "    if 'entropy' not in c[1]:\n",
    "        non_entropy_col_indices.append(i)\n",
    "\n",
    "\n",
    "# y_test\n",
    "y_labels = [x in active_test_users for x in test_users]\n",
    "y_df = pd.DataFrame({\n",
    "    'user': test_users,\n",
    "    'label': y_labels # 1 for active\n",
    "})\n",
    "y_df.set_index('user', inplace=True)\n",
    "\n",
    "# Concat two types of features\n",
    "test_df = pd.concat([\n",
    "    test_df, \n",
    "    test_df_spmf, y_df], axis=1)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "# X_test\n",
    "X_test_ = test_df.values[:,:-1]\n",
    "y_test = test_df.values[:,-1]\n",
    "y_test = 1-y_test # 1 for inactive\n",
    "np.nan_to_num(X_test_, copy=False, nan=0)\n",
    "\n",
    "###\n",
    "X_test_ = X_test_.astype(dtype=np.float32)\n",
    "y_test = y_test.astype(dtype=np.int8)\n",
    "# X_train = np.log10(X_train_+1.)\n",
    "X_test = X_test_\n",
    "X_test[:,:62] = np.log10(X_test[:,:62]+1.) # only log on non-spmf features\n",
    "print(X_test.shape)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746327c",
   "metadata": {},
   "source": [
    "Step2: Training DeepFM-Pattern\n",
    "--------------\n",
    "(Go to Step3: Testing DeepFM-Pattern to run the one already trained for the paper)\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a2cd635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of dense features 0\n",
      "# of params 57485\n",
      "Epoch 1/1000\n",
      "190/190 - 37s - loss: 0.4566 - auc: 0.8671 - val_loss: 0.4440 - val_auc: 0.8747\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.87469, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 2/1000\n",
      "190/190 - 13s - loss: 0.4311 - auc: 0.8809 - val_loss: 0.4385 - val_auc: 0.8763\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.87469 to 0.87626, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 3/1000\n",
      "190/190 - 12s - loss: 0.4288 - auc: 0.8816 - val_loss: 0.4356 - val_auc: 0.8772\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.87626 to 0.87720, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 4/1000\n",
      "190/190 - 13s - loss: 0.4271 - auc: 0.8824 - val_loss: 0.4372 - val_auc: 0.8773\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.87720 to 0.87733, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 5/1000\n",
      "190/190 - 13s - loss: 0.4265 - auc: 0.8826 - val_loss: 0.4365 - val_auc: 0.8774\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.87733 to 0.87740, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 6/1000\n",
      "190/190 - 13s - loss: 0.4257 - auc: 0.8829 - val_loss: 0.4358 - val_auc: 0.8778\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.87740 to 0.87776, saving model to DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
      "Epoch 7/1000\n",
      "190/190 - 13s - loss: 0.4255 - auc: 0.8832 - val_loss: 0.4357 - val_auc: 0.8773\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.87776\n",
      "Epoch 8/1000\n",
      "190/190 - 14s - loss: 0.4253 - auc: 0.8831 - val_loss: 0.4347 - val_auc: 0.8775\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.87776\n",
      "Epoch 9/1000\n",
      "190/190 - 13s - loss: 0.4244 - auc: 0.8836 - val_loss: 0.4371 - val_auc: 0.8777\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.87776\n",
      "Epoch 10/1000\n",
      "190/190 - 13s - loss: 0.4245 - auc: 0.8835 - val_loss: 0.4363 - val_auc: 0.8773\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.87776\n",
      "Epoch 11/1000\n",
      "190/190 - 17s - loss: 0.4235 - auc: 0.8840 - val_loss: 0.4376 - val_auc: 0.8776\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.87776\n",
      "Epoch 12/1000\n",
      "190/190 - 14s - loss: 0.4233 - auc: 0.8841 - val_loss: 0.4357 - val_auc: 0.8773\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.87776\n",
      "Epoch 13/1000\n",
      "190/190 - 13s - loss: 0.4231 - auc: 0.8843 - val_loss: 0.4380 - val_auc: 0.8770\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.87776\n",
      "Epoch 14/1000\n",
      "190/190 - 14s - loss: 0.4230 - auc: 0.8843 - val_loss: 0.4359 - val_auc: 0.8774\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.87776\n",
      "Epoch 15/1000\n",
      "190/190 - 13s - loss: 0.4226 - auc: 0.8845 - val_loss: 0.4358 - val_auc: 0.8772\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.87776\n",
      "Epoch 16/1000\n",
      "190/190 - 13s - loss: 0.4220 - auc: 0.8849 - val_loss: 0.4366 - val_auc: 0.8771\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.87776\n",
      "CPU times: user 7min 14s, sys: 6min 22s, total: 13min 37s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_spmfsparse_r1.h5'\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns = ['f'+str(i) for i in range(X_train.shape[1])])\n",
    "\n",
    "###\n",
    "dense_features = X_train_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "train_model_input = {name: X_train_df[name].values for name in dense_features}\n",
    "###\n",
    "\n",
    "### sparse features for DeepFM for patterns\n",
    "sparse_features = [x for x in X_train_df.columns[62:]]\n",
    "# don't use stat features\n",
    "dense_features = [] \n",
    "print('# of dense features', len(dense_features))\n",
    "X_train_df[sparse_features] = X_train_df[sparse_features].astype(int)\n",
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=X_train_df[feat].max() + 1,embedding_dim=4)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "train_model_input = {name: X_train_df[name].values for name in feature_names}\n",
    "###\n",
    "\n",
    "\n",
    "##### define model\n",
    "tensorflow.keras.backend.clear_session()\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', seed=28)\n",
    "print('# of params',model.count_params())\n",
    "\n",
    "# compiling the model\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', patience=10)\n",
    "mc = ModelCheckpoint(model_weight_path, \n",
    "                     monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[AUC()], )\n",
    "\n",
    "# training the model\n",
    "history = model.fit(train_model_input, y_train, batch_size=256, epochs=1000, \n",
    "                    validation_split=0.2, verbose=2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ace2e",
   "metadata": {},
   "source": [
    "Step3: Testing DeepFM-Pattern\n",
    "------------------------\n",
    "The trained model weights for the 5 runs for the main article are in the same folder, which you can use for setting ```model_weight_path``` varialble in the cell below. \n",
    "- DeepFM_w_seed28_b1_spmfsparse_r1.h5\n",
    "- DeepFM_w_seed28_b1_spmfsparse_r2.h5\n",
    "- DeepFM_w_seed28_b1_spmfsparse_r3.h5\n",
    "- DeepFM_w_seed28_b1_spmfsparse_r4.h5\n",
    "- DeepFM_w_seed28_b1_spmfsparse_r5.h5\n",
    "  \n",
    "In case of using newly trained model weight, you can use set the same model_weight_path in the Step2: Training DeepFM\n",
    "- DeepFM_w_seed28_b1_spmfsparse.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f7ddbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65568, 140)\n",
      "# of features 78\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7838    0.8104    0.7969     32068\n",
      "           1     0.8124    0.7861    0.7990     33500\n",
      "\n",
      "    accuracy                         0.7980     65568\n",
      "   macro avg     0.7981    0.7982    0.7979     65568\n",
      "weighted avg     0.7984    0.7980    0.7980     65568\n",
      "\n",
      "AUROC 0.8787050637730643\n",
      "Log Loss 0.4314696413724143\n",
      "0.7838\t0.8104\t0.7969\t0.8124\t0.7861\t0.7990\t0.8787050637730643\t0.7980\t0.4314696413724143\n",
      "CPU times: user 12.3 s, sys: 7.4 s, total: 19.7 s\n",
      "Wall time: 9.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_weight_path = 'DeepFM_w_seed28_b1_spmfsparse_r5.h5'\n",
    "\n",
    "### DeepFM load and test\n",
    "X_test_df = pd.DataFrame(X_test, columns = ['f'+str(i) for i in range(X_test.shape[1])])\n",
    "print(X_test_df.shape)\n",
    "\n",
    "dense_features = X_test_df.columns\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "test_model_input = {name: X_test_df[name].values for name in dense_features}\n",
    "###\n",
    "\n",
    "\n",
    "### sparse features for DeepFM for patterns\n",
    "sparse_features = [x for x in X_test_df.columns[62:]]\n",
    "# don't use stat features\n",
    "dense_features = []\n",
    "X_test_df[sparse_features] = X_test_df[sparse_features].astype(int)\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=X_test_df[feat].max() + 1,embedding_dim=4)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "print('# of features', len(feature_names))\n",
    "test_model_input = {name: X_test_df[name].values for name in feature_names}\n",
    "###\n",
    "\n",
    "clf = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', seed=28)\n",
    "clf.load_weights(model_weight_path)\n",
    "y_prob = clf.predict(test_model_input, batch_size=256).ravel()\n",
    "y_pred = [int(x) for x in (y_prob>=.5)]\n",
    "# print(y_pred)\n",
    "# print(y_prob)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob, pos_label=1)\n",
    "print('AUROC', metrics.auc(fpr, tpr))\n",
    "print('Log Loss', metrics.log_loss(y_test, y_prob, eps=1e-7))\n",
    "\n",
    "# easy to paste version\n",
    "res = classification_report(y_test, y_pred, digits=4).split('\\n')\n",
    "label0 = [x for x in res[2].split(' ') if len(x)>1][:3]\n",
    "label1 = [x for x in res[3].split(' ') if len(x)>1][:3]\n",
    "acc = [x for x in res[5].split(' ') if len(x)>1][1]\n",
    "res = label0+label1+[str(metrics.auc(fpr, tpr)), str(acc), str(metrics.log_loss(y_test, y_prob))]\n",
    "print('\\t'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc303509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7tf2.4.1",
   "language": "python",
   "name": "py3.7tf2.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
